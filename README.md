# This notebook documents the step by step process and python codes used in vibe-coding.
## Goal: Teach the computer to tell emotions from texts

### Step 1: The setup
Weâ€™ll need a library called TextBlob â€” itâ€™s like a tiny language helper that can â€œfeelâ€ the mood of words.


[2]
4s
# install the library (you run this once)
# pip install textblob

from textblob import TextBlob
### Step 2: Give it some sentences to analyze

[5]
0s
sentences = [
    "Iâ€™m feeling amazing today!",
    "I hate waking up early.",
    "What a beautiful morning!",
    "Iâ€™m so tired and bored.",
    "This is the best day ever!"
]
### Step 3: Let the AI check the vibe


[4]
0s
for text in sentences:
    blob = TextBlob(text)
    mood_score = blob.sentiment.polarity  # ranges from -1 (sad) to +1 (happy)
    
    if mood_score > 0:
        vibe = "ðŸ˜Š Positive Vibe"
    elif mood_score < 0:
        vibe = "ðŸ˜¢ Negative Vibe"
    else:
        vibe = "ðŸ˜ Neutral Vibe"
    
    print(f"{text} --> {vibe} (score: {mood_score})")
Iâ€™m feeling amazing today! --> ðŸ˜Š Positive Vibe (score: 0.7500000000000001)
I hate waking up early. --> ðŸ˜¢ Negative Vibe (score: -0.35000000000000003)
What a beautiful morning! --> ðŸ˜Š Positive Vibe (score: 1.0)
Iâ€™m so tired and bored. --> ðŸ˜¢ Negative Vibe (score: -0.45)
This is the best day ever! --> ðŸ˜Š Positive Vibe (score: 1.0)
Whatâ€™s happening behind the scenes:

â€¢   TextBlob looks at the words and how theyâ€™re used.
â€¢   It knows that â€œamazing,â€ â€œbeautiful,â€ and â€œbestâ€ are positive words.
â€¢   It also knows â€œhate,â€ â€œtired,â€ and â€œboredâ€ usually mean negative feelings.
â€¢   Then it gives each sentence a â€œvibe score.â€
Thatâ€™s vibe coding in baby form â€” detecting emotional tone from words.

To make this vibe detector a little smarter â€” for example, one that can detect sarcasm or mixed emotions
Now weâ€™re moving from a baby vibe detector â†’ to a teenage vibe detector â€” one that can spot sarcasm, mixed emotions, and stronger mood swings.

### Step 1: The Problem
Basic vibe coding (like TextBlob) only looks at positive or negative words, but humans are trickier.

Examples:

â€œOh great, another Monday.â€ â€œYeah, I totally love doing homework.â€

Those sound positive on the surface (â€œgreat,â€ â€œloveâ€)â€¦ but the vibe is clearly sarcastic ðŸ˜’

So we need an AI that understands context â€” not just words.

### Step 2: Bring in a Smarter Brain (Transformers ðŸ¦¾)
Modern AI uses transformer models like BERT, RoBERTa, or DistilBERT â€” these are like super brains trained on millions of examples of real human speech, tweets, reviews, etc.

They can â€œfeelâ€ tone, detect sarcasm, and sense emotional complexity.

We can use a ready-made one from a library called Hugging Face Transformers.


[7]
2s
# install first if needed
# pip install transformers torch

from transformers import pipeline

# load a pre-trained emotion detection model
emotion_analyzer = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=False)

# try it out!
sentences = [
    "Oh great, another Monday.",
    "I canâ€™t believe I failed again ðŸ˜”",
    "Iâ€™m so proud of myself today!",
    "Sure, because everything always works perfectly (eye roll).",
    "That movie was sad but kind of beautiful."
]

for text in sentences:
    result = emotion_analyzer(text)[0]
    print(f"{text} â†’ {result['label']} ({result['score']:.2f})")
Device set to use cpu
Oh great, another Monday. â†’ joy (0.92)
I canâ€™t believe I failed again ðŸ˜” â†’ sadness (0.74)
Iâ€™m so proud of myself today! â†’ joy (0.85)
Sure, because everything always works perfectly (eye roll). â†’ neutral (0.82)
That movie was sad but kind of beautiful. â†’ sadness (0.97)
Results aren't exactly what we're expecting because the model we used â€” j-hartmann/emotion-english-distilroberta-base â€” is trained mainly on explicit emotions (like joy, sadness, anger, fear, disgust, etc.), not sarcasm.

So when it sees:

â€œOh great, another Monday.â€ it notices the word â€œgreat,â€ which is usually positive, and doesnâ€™t fully catch the sarcastic tone â€” because it lacks vocal or situational clues.

Hence:

â†’ joy (0.92)

The model isnâ€™t â€œwrongâ€ â€” itâ€™s doing what it was trained for â€” but itâ€™s not vibe-aware enough yet.

To make our results more vibe-aware we can do either of the following:

Use a model trained specifically for sarcasm
Use a multi-label emotion model (some models can say multiple vibes at once - like "sad, but hopeful")
Combine Models (Vibe stacking)
Real-world systems (like Spotify or TikTok) often combine: â€¢ Emotion model (text emotion) â€¢ Sarcasm model â€¢ Sentiment model â€¢ Context model (user history or audio tone)

Together, that fusion gives a much stronger â€œvibe sense.â€

For the purpose of this exercise, we would try out a multi-label emotion model


[9]
42s
# install first if needed
# pip install transformers torch

from transformers import pipeline

# load a pre-trained emotion detection model
from transformers import pipeline
sarcasm_detector = pipeline("text-classification", model="mrm8488/t5-base-finetuned-sarcasm-twitter")
sarcasm_detector("Oh great, another Monday.")
# try it out!
sentences = [
    "Oh great, another Monday.",
    "I canâ€™t believe I failed again ðŸ˜”",
    "Iâ€™m so proud of myself today!",
    "Sure, because everything always works perfectly (eye roll).",
    "That movie was sad but kind of beautiful."
]

for text in sentences:
    result = emotion_analyzer(text)[0]
    print(f"{text} â†’ {result['label']} ({result['score']:.2f})")

Why sarcasm is hard for AI

Sarcasm needs context, tone, or history â€” things text alone canâ€™t fully show.

For example:

â€œWow, youâ€™re so earlyâ€¦â€ If the person actually came late, the vibe is sarcastic. But unless the AI knows that context, it assumes â€œwowâ€ + â€œsoâ€ + â€œearlyâ€ = positive.

Thatâ€™s why sarcasm detection is an advanced branch of vibe coding. It often needs: More context (previous sentences, speaker style) Tone of voice (audio input) Or models trained on sarcastic datasets (like â€œTwitter Sarcasm Corpusâ€)

Following this train of thought we would like to see how we could take this same idea and make a music or video vibe detector next, so the AI can feel the mood of a song or clip, not just text.
Step 1: The New Problem â€” Feeling the Mood of Music & Video
Imagine these:

ðŸŽµ A slow piano ballad â†’ feels melancholy, even if no lyrics.

ðŸŽ¥ A fast-cut action trailer with deep bass â†’ feels intense or thrilling.

ðŸŽ¶ A pop song with major chords and upbeat tempo â†’ feels joyful.

ðŸ§ A TikTok clip where someone is dancing but the music is ironic â†’ mixed vibes.

Unlike text, here the â€œvibeâ€ lives in sound + visuals + lyrics. So, our vibe detector must understand:

ðŸ”Š Audio features â†’ tempo, pitch, energy, melody

ðŸ“ Lyrics (if present) â†’ using the text vibe detector we built

ðŸ‘ Visual cues â†’ colors, brightness, facial expressions, motion

Step 2: Bring in Audio/Video Brains
Modality	Common Tools / Models
Audio	OpenL3 (audio embeddings), YAMNet (sound classification), Musicnn (genre/mood), Wav2Vec2 (speech)
Lyrics/Text	Transformers (like before)
Visuals	CLIP (image+text), OpenCV (basic), or ViT (Vision Transformer) for emotion scenes
We can combine these into a multimodal vibe pipeline

Step 3: A Mini Music Vibe Detector (Demo Code)
Below is a simplified Python example using ðŸŸ¡ librosa to get audio features + a simple rule-based mood guess. (You could later swap the â€œmood detectorâ€ with a real model like musicnn.)

ðŸ“ This focuses on instrumental audio mood, not lyrics.


[11]
0s
# ðŸŽ§ Mini Music Vibe Detector (Beginner Demo)
# pip install librosa numpy

import librosa
import numpy as np

def detect_music_vibe(audio_path):
    # 1ï¸âƒ£ Load the audio file
    y, sr = librosa.load(audio_path, duration=60)  # load first 60 seconds

    # 2ï¸âƒ£ Extract basic features
    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
    energy = np.mean(librosa.feature.rms(y=y))
    brightness = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))

    # 3ï¸âƒ£ Simple vibe classification rules
    if tempo > 120 and energy > 0.05:
        vibe = "ðŸŽ‰ Upbeat / Energetic"
    elif brightness < 2000 and energy < 0.03:
        vibe = "ðŸŒ§ Sad / Calm / Reflective"
    else:
        vibe = "ðŸŽ¶ Mixed / Neutral Mood"

    return {
        "tempo": round(tempo, 2),
        "energy": float(energy),
        "brightness": float(brightness),
        "detected_vibe": vibe
    }

This little demo extracts tempo, energy, and brightness â€” super simple audio mood cues. Later, you can plug in models like:

musicnn â†’ pre-trained for music mood/genre.

OpenL3 â†’ to embed audio â†’ classify moods with a custom model.

Step 4: Video Vibe Detector (Concept)
For videos, you combine:

Audio track â†’ use the music detector above

Transcript (speech) â†’ use a speech-to-text model like Whisper â†’ then feed the text to our teenage vibe detector ðŸ§ 

Frames â†’ sample 1 frame per second â†’ run through a Vision Transformer (ViT) or CLIP to detect scene emotion (e.g., bright colors, facial expressions, action).

Mini demo for video frames (concept):


[12]
0s
# pip install opencv-python
import cv2

def extract_frames(video_path, frame_rate=1):
    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    frame_interval = int(fps / frame_rate)
    frames = []
    i = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if i % frame_interval == 0:
            frames.append(frame)
        i += 1
    cap.release()
    return frames

frames = extract_frames("clip.mp4")
print(f"Extracted {len(frames)} frames for vibe analysis ðŸ–¼")

Extracted 0 frames for vibe analysis ðŸ–¼
Youâ€™d then send these frames into a CLIP model or emotion classifier (e.g., â€œhappy scene,â€ â€œtense,â€ â€œromantic,â€ â€œdarkâ€), and blend that with the audio+text analysis.

Step 5: Fuse All the Vibes
Finally, we combine the scores from:

ðŸŽ¶ Audio mood

ðŸ“ Lyrics / Speech vibe

ðŸ‘ Visual scene vibe

Example (pseudo):


[14]
0s
# final_vibe = weighted_average([audio_vibe, text_vibe, visual_vibe])

# This is a conceptual representation of combining different vibe scores.
# In a real implementation, you would define how to combine the scores
# from audio, text, and visual analysis (e.g., using numerical scores and weights).
There are already some multimodal models you can experiment with:

ðŸ§  CLAP (Contrastive Language-Audio Pretraining) â†’ audio + text mood understanding

ðŸ§  VideoCLIP / ViViT â†’ video mood/scene analysis

ðŸ§  AudioSpectrogram Transformers â†’ detect music genre, mood, or emotion directly.

In summary:
Text â†’ Teenage vibe detector ðŸ§ 

Music â†’ Adds emotional rhythm and sonic energy ðŸŽ§

Video â†’ Adds visual emotion and atmosphere ðŸ‘

âž• Fusion = A full sensory vibe intelligence ðŸŒˆðŸ¤–

